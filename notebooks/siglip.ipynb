{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import torch\n",
    "from tokenizers import Tokenizer\n",
    "from safetensors import safe_open\n",
    "\n",
    "from model.siglip import SiglipModel, SiglipConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config: SiglipConfig = {\n",
    "    \"text_config\": {\n",
    "    \"hidden_size\": 1152,\n",
    "    \"intermediate_size\": 4304,\n",
    "    \"num_attention_heads\": 16,\n",
    "    \"num_hidden_layers\": 27,\n",
    "    \"vocab_size\": 32000,\n",
    "    \"max_position_embeddings\": 64,\n",
    "    \"attention_dropout\": 0.0,\n",
    "    \"layer_norm_eps\": 1e-6,\n",
    "  },\n",
    "  \"vision_config\": {\n",
    "    \"hidden_size\": 1152,\n",
    "    \"image_size\": 384,\n",
    "    \"intermediate_size\": 4304,\n",
    "    \"num_attention_heads\": 16,\n",
    "    \"num_hidden_layers\": 27,\n",
    "    \"patch_size\": 14,\n",
    "    \"num_channels\": 3,\n",
    "    \"attention_dropout\": 0.0,\n",
    "    \"layer_norm_eps\": 1e-6,\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SiglipModel(config=config)\n",
    "tokenizer = Tokenizer.from_file(os.path.join('../weights/siglip/', 'tokenizer.json'))\n",
    "tokenizer.enable_padding(pad_id=1, length=config['text_config']['max_position_embeddings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_dict_from_safetensors(path: str | list[str], device: torch.device = torch.device('cpu'), dtype: torch.dtype = torch.bfloat16) -> dict:\n",
    "    state_dict = {}\n",
    "    if isinstance(path, str): path = [path]\n",
    "    if path:\n",
    "        d = device.type if device.type == 'cpu' else device.index\n",
    "        for p in path:\n",
    "            with safe_open(p, framework=\"pt\", device=d) as f:\n",
    "                for k in f.keys(): state_dict[k] = f.get_tensor(k).to(dtype=dtype)\n",
    "    else: print(\"No weights found.\")\n",
    "    return state_dict\n",
    "\n",
    "sd = get_state_dict_from_safetensors('../weights/siglip/model.safetensors')\n",
    "model.load_state_dict(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from typing import Union\n",
    "import requests\n",
    "\n",
    "def preprocess_image(image_input: Union[str, bytes, Image.Image], image_size: int = 384):\n",
    "    if isinstance(image_input, Image.Image): image = image_input\n",
    "    elif isinstance(image_input, bytes): image = Image.open(BytesIO(image_input))\n",
    "    elif image_input.startswith('http'): image = Image.open(requests.get(image_input, stream=True).raw)\n",
    "    else: image = Image.open(image_input)\n",
    "    \n",
    "    if image.mode == 'RGBA':\n",
    "        image = image.convert('RGB')\n",
    "\n",
    "    num_channels = len(image.getbands())\n",
    "    \n",
    "    normalize_transform = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5] * num_channels, std=[0.5] * num_channels)\n",
    "    ])\n",
    "    \n",
    "    tensor_image = normalize_transform(image)\n",
    "    tensor_image = tensor_image.unsqueeze(0)\n",
    "    return tensor_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"a photo of earth from moon\", \"a photo of 2 people on moon\", \"2 people sitting on moon\"]\n",
    "inputs = {\n",
    "    'input_ids': torch.tensor([t.ids for t in tokenizer.encode_batch(texts,)]),\n",
    "    'pixel_values': preprocess_image(\"/home/andrew264/Downloads/Screenshot_23.png\"),\n",
    "    }\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "logits = outputs[0]\n",
    "probs = torch.sigmoid(logits)\n",
    "for i, text in enumerate(texts):\n",
    "    print(f\"{probs[0][i]:.1%} that image is '{text}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "config: SiglipConfig = {\n",
    "        \"text_config\": {\n",
    "        \"hidden_size\": 768,\n",
    "        \"intermediate_size\": 3072,\n",
    "        \"num_attention_heads\": 12,\n",
    "        \"num_hidden_layers\": 12,\n",
    "        \"vocab_size\": 10,\n",
    "        \"max_position_embeddings\": 1,\n",
    "        \"attention_dropout\": 0.0,\n",
    "        \"layer_norm_eps\": 1e-6,\n",
    "        },\n",
    "    \"vision_config\": {\n",
    "        \"image_size\": 224,\n",
    "        \"hidden_size\": 768,\n",
    "        \"intermediate_size\": 3072,\n",
    "        \"num_attention_heads\": 12,\n",
    "        \"num_hidden_layers\": 12,\n",
    "        \"patch_size\": 16,\n",
    "        \"num_channels\": 1,\n",
    "        \"attention_dropout\": 0.0,\n",
    "        \"layer_norm_eps\": 1e-6,\n",
    "        }\n",
    "    }\n",
    "model = SiglipModel(config=config)\n",
    "model.load_state_dict(torch.load('../weights/siglip/siglip.pt', map_location='cpu', weights_only=True))\n",
    "dataset = load_dataset(\"ylecun/mnist\", split='test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    'input_ids': torch.tensor([[i] for i in range(10)]),\n",
    "    'pixel_values': preprocess_image(dataset[0]['image'], image_size=28),\n",
    "    }\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "logits = outputs[0]\n",
    "probs = torch.sigmoid(logits)\n",
    "for i, text in enumerate(range(10)):\n",
    "    print(f\"{probs[0][i]:.1%} that image is '{text}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
